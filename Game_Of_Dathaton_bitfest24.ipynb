{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtF9r2Nv_E5P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re,json,nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/content/train.csv')\n",
        "test=pd.read_csv('/content/test.csv')\n",
        "sub=pd.read_csv('/content/sample_submission.csv')"
      ],
      "metadata": {
        "id": "thBczLmg_00F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "rby8kJPcAEGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "MVI2RoMME52u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "_GK_bSrCFJZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()/train.shape[0]*100"
      ],
      "metadata": {
        "id": "JdUwGFK5Fx0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "N7IuqrIDHbCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "opZWkDfwIBR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace [None] and 'N/A' with NaN\n",
        "for col in train.columns:\n",
        "    train[col] = train[col].replace({np.nan: None, '[None]': None, 'N/A': None}) # Remove ['N/A'] from replace\n",
        "\n",
        "for col in test.columns:\n",
        "    test[col] = test[col].replace({np.nan: None, '[None]': None, 'N/A': None}) # Remove ['N/A'] from replace\n",
        "\n",
        "for col in sub.columns:\n",
        "    sub[col] = sub[col].replace({np.nan: None, '[None]': None, 'N/A': None}) # Remove ['N/A'] from replace\n",
        "\n",
        "train.info()"
      ],
      "metadata": {
        "id": "O1k7cw2rl8t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(train.isnull())"
      ],
      "metadata": {
        "id": "mlPVCD0d8FhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_percentage = train.isnull().sum() * 100 / len(train)\n",
        "print(null_percentage)"
      ],
      "metadata": {
        "id": "8U_DTQzd8L_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with high null percentage as indicated\n",
        "columns_to_drop = ['extra_curricular_activity_types', 'extra_curricular_organization_names',\n",
        "                   'extra_curricular_organization_links', 'role_positions', 'languages',\n",
        "                   'proficiency_levels', 'certification_providers', 'certification_skills',\n",
        "                   'online_links', 'issue_dates', 'expiry_dates', 'address']\n",
        "\n",
        "train = train.drop(columns=columns_to_drop, errors='ignore')\n",
        "test = test.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Display info of the modified DataFrames (optional)\n",
        "train.info()\n",
        "test.info()"
      ],
      "metadata": {
        "id": "U5rLVGsonTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = train[train.duplicated()]\n",
        "print(duplicate_rows)"
      ],
      "metadata": {
        "id": "iDaKDDD1goI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "lNI-OEyKgw89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "NivNvAp5g1pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): #check if text is nan if so return it without changing\n",
        "        return text\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text)) #convert to string here\n",
        "    text = re.sub(r'\\[|\\]', '', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    text = \" \".join(words)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train[col] = train[col].apply(clean_text)\n",
        "\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    test[col] = test[col].apply(clean_text)"
      ],
      "metadata": {
        "id": "dnlYzwAKhrJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply steamming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "try:\n",
        "    PorterStemmer()\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train[col] = train[col].apply(stem_text)\n",
        "\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    test[col] = test[col].apply(stem_text)"
      ],
      "metadata": {
        "id": "zf7hsmALrjMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "176RnZP1kiu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Install the NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets') # This downloads a list of available tagsets, including 'eng'\n",
        "nltk.download('averaged_perceptron_tagger_eng') # This line is added to download 'averaged_perceptron_tagger_eng'\n",
        "\n",
        "def add_tokens(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return \" \".join([f\"{word}/{tag}\" for word, tag in pos_tags])\n",
        "\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train[col] = train[col].apply(add_tokens)\n",
        "\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    test[col] = test[col].apply(add_tokens)"
      ],
      "metadata": {
        "id": "KaSFTt8PwsRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install the NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets') # This downloads a list of available tagsets, including 'eng'\n",
        "nltk.download('averaged_perceptron_tagger_eng') # This line is added to download 'averaged_perceptron_tagger_eng'\n",
        "nltk.download('punkt_tab') # Download the 'punkt_tab' data package\n",
        "\n",
        "def add_tokens(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return \" \".join([f\"{word}/{tag}\" for word, tag in pos_tags])\n",
        "\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train[col] = train[col].apply(add_tokens)\n",
        "\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    test[col] = test[col].apply(add_tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-eNyagw5kwDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "U73MfU-RiYI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_percentage = train.isnull().sum() * 100 / len(train)\n",
        "print(null_percentage)"
      ],
      "metadata": {
        "id": "vA9hFbVGpNq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split"
      ],
      "metadata": {
        "id": "4C7Ps7vBneW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Label Encoding with Null Value Preservation\n",
        "# def label_encode_with_nulls(train_df, test_df, column):\n",
        "#     le = LabelEncoder()\n",
        "\n",
        "#     # Fit the encoder on the training data, handling nulls\n",
        "#     train_values = train_df[column].dropna()\n",
        "#     le.fit(train_values)\n",
        "\n",
        "#     # Transform both train and test data, preserving nulls\n",
        "#     train_df[column] = train_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "#     test_df[column] = test_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "#     return train_df, test_df\n",
        "\n",
        "# # Apply label encoding to object columns, preserving nulls\n",
        "# for col in train.select_dtypes(include=['object']).columns:\n",
        "#     train, test = label_encode_with_nulls(train, test, col)\n",
        "\n",
        "# # ... (rest of your code)"
      ],
      "metadata": {
        "id": "psOntCrkpUIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Label Encoding with Null Value Preservation\n",
        "def label_encode_with_nulls(train_df, test_df, column):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the encoder on the training data, handling nulls\n",
        "    # Combine unique values from both train and test to ensure all labels are seen\n",
        "    all_values = pd.concat([train_df[column], test_df[column]]).dropna().unique()\n",
        "    le.fit(all_values) # Fit on combined unique values\n",
        "\n",
        "    # Transform both train and test data, preserving nulls\n",
        "    train_df[column] = train_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "    test_df[column] = test_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "    return train_df, test_df\n",
        "\n",
        "# Apply label encoding to object columns, preserving nulls\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train, test = label_encode_with_nulls(train, test, col)\n",
        "\n",
        "# ... (rest of your code)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "APY9cdXruluV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_percentage = train.isnull().sum() * 100 / len(train)\n",
        "print(null_percentage)"
      ],
      "metadata": {
        "id": "Qg2pWw0v8YBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "NMpPwIAf8eXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fill missing values in train dataset with the mean of each column\n",
        "# for col in train.columns:\n",
        "#     if train[col].isnull().any():\n",
        "#         mean_val = train[col].mean()\n",
        "#         train[col].fillna(mean_val, inplace=True)\n",
        "\n",
        "# # Fill missing values in test dataset with the mean of each column\n",
        "# for col in test.columns:\n",
        "#     if test[col].isnull().any():\n",
        "#         mean_val = test[col].mean()\n",
        "#         test[col].fillna(mean_val, inplace=True)"
      ],
      "metadata": {
        "id": "w7CDV4SO8jvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Fill missing values in train dataset with the mean of each column\n",
        "for col in train.columns:\n",
        "    if train[col].isnull().any():\n",
        "        if pd.api.types.is_numeric_dtype(train[col]): # Check if column is numeric\n",
        "            mean_val = train[col].mean()\n",
        "            train[col].fillna(mean_val, inplace=True)\n",
        "        else:\n",
        "            # Handle non-numeric columns (e.g., fill with mode or a constant)\n",
        "            mode_val = train[col].mode()[0]  # Get the mode\n",
        "            train[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "# Fill missing values in test dataset with the mean of each column\n",
        "for col in test.columns:\n",
        "    if test[col].isnull().any():\n",
        "        if pd.api.types.is_numeric_dtype(test[col]): # Check if column is numeric\n",
        "            mean_val = test[col].mean()\n",
        "            test[col].fillna(mean_val, inplace=True)\n",
        "        else:\n",
        "            # Handle non-numeric columns (e.g., fill with mode or a constant)\n",
        "            mode_val = test[col].mode()[0]  # Get the mode\n",
        "            test[col].fillna(mode_val, inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LowEdaHMvFtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_percentage = train.isnull().sum() * 100 / len(train)\n",
        "print(null_percentage)"
      ],
      "metadata": {
        "id": "IPbaP-h98lzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "6u5QfBPEvLbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=train.drop('matched_score', axis=1)\n",
        "x"
      ],
      "metadata": {
        "id": "5fpMZycc8n_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=train['matched_score']\n",
        "y"
      ],
      "metadata": {
        "id": "mHNaOD0e8vAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnmXToJyl7Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "i7yg-_tcl7Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning**"
      ],
      "metadata": {
        "id": "7TdqX_isl7ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "xvfk2dM_oDMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "cp16KApqoQ7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN"
      ],
      "metadata": {
        "id": "kuCVGlh4mF83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ANN Model\n",
        "# ann_model = Sequential([\n",
        "#     Dense(64, input_dim=x_train.shape[1], activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(32, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(y_train.shape[1], activation='softmax')  # Adjust for the number of classes\n",
        "# ])\n",
        "\n",
        "# ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# ann_model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "kvC9ctuQmAnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# One-hot encode y_train\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# ANN Model\n",
        "ann_model = Sequential([\n",
        "    Dense(64, input_dim=x_train.shape[1], activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Now y_train.shape[1] will be the number of classes\n",
        "])\n",
        "\n",
        "ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "ann_model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Y8RCiBpmvaXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# One-hot encode y_train\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# ANN Model\n",
        "ann_model = Sequential([\n",
        "    Dense(64, input_dim=x_train.shape[1], activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Now y_train.shape[1] will be the number of classes\n",
        "])\n",
        "\n",
        "ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "ann_model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DhPUgI0Voeqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "HhmjBYZ0mL_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for RNN (3D input: samples, timesteps, features)\n",
        "x_train_rnn = np.expand_dims(x_train, axis=1)\n",
        "x_test_rnn = np.expand_dims(x_test, axis=1)"
      ],
      "metadata": {
        "id": "NcMMBfIkozEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for RNN (3D input: samples, timesteps, features)\n",
        "X_train_rnn = np.expand_dims(x_train, axis=1)\n",
        "X_test_rnn = np.expand_dims(x_test, axis=1)\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(64, input_shape=(x_train_rnn.shape[1], x_train_rnn.shape[2]), activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "rnn_model.fit(x_train_rnn, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "HvW2i1MVmNv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "SDulD3fhmQXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(x_train_rnn.shape[1], x_train_rnn.shape[2]), activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(x_train_rnn, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "3o7A1baImR-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ANN\n",
        "ann_loss, ann_accuracy = ann_model.evaluate(x_test, y_test)\n",
        "print(f\"ANN Accuracy: {ann_accuracy:.2f}\")\n",
        "\n",
        "# Evaluate RNN\n",
        "rnn_loss, rnn_accuracy = rnn_model.evaluate(x_test_rnn, y_test)\n",
        "print(f\"RNN Accuracy: {rnn_accuracy:.2f}\")\n",
        "\n",
        "# Evaluate LSTM\n",
        "lstm_loss, lstm_accuracy = lstm_model.evaluate(x_test_rnn, y_test)\n",
        "print(f\"LSTM Accuracy: {lstm_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "kLo2CLvMmVnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ANN\n",
        "# Predictions for calculating MSE\n",
        "from sklearn.metrics import mean_squared_error  # Import mean_squared_error\n",
        "y_test_pred_ANN = ann_model.predict(x_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_ANN = mean_squared_error(y_test, y_test_pred_ANN)\n",
        "print(f\"Mean Squared Error: {mse_ANN:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tjrSxHeSp6-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN\n",
        "# Predictions for calculating MSE\n",
        "from sklearn.metrics import mean_squared_error  # Import mean_squared_error\n",
        "y_train_pred_ANN = ann_model.predict(x_train)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_ANN_train = mean_squared_error(y_train, y_train_pred_ANN)\n",
        "print(f\"Mean Squared Error: {mse_ANN_train:.4f}\")"
      ],
      "metadata": {
        "id": "f0fHwALImbPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# RNN\n",
        "# Predictions for calculating MSE\n",
        "y_train_RNN = rnn_model.predict(np.expand_dims(x_train, axis=1)) # Reshape x_train to add timesteps dimension\n",
        "\n",
        "# Calculate MSE\n",
        "mse_RNN = mean_squared_error(y_train, y_train_RNN)\n",
        "print(f\"Mean Squared Error: {mse_RNN:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Dh3hi5HxqoMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN\n",
        "# Predictions for calculating MSE\n",
        "y_test_RNN = rnn_model.predict(np.expand_dims(x_test, axis=1)) # Reshape x_train to add timesteps dimension\n",
        "\n",
        "# Calculate MSE\n",
        "mse_RNN_test = mean_squared_error(y_test, y_test_RNN)\n",
        "print(f\"Mean Squared Error: {mse_RNN_test:.4f}\")"
      ],
      "metadata": {
        "id": "zNtU-JajqtsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # Predictions for calculating MSE\n",
        "# y_train_pred = lstm_model.predict(x_train)\n",
        "\n",
        "# # Calculate MSE\n",
        "# mse = mean_squared_error(y_train, y_train_pred)\n",
        "# print(f\"Mean Squared Error: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "TtZlhnxFpMJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# LSTM\n",
        "# Predictions for calculating MSE\n",
        "y_train_pred = lstm_model.predict(np.expand_dims(x_train, axis=1)) # Reshape x_train to add timesteps dimension\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_train, y_train_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SS3m7kWPrJFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "lPORvsATsIcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "0YQPIUjVraFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission File"
      ],
      "metadata": {
        "id": "nDIehFnbtGxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract and store the \"ID\" column from the \"test\" dataset\n",
        "test_ids = test['ID']  # Store the \"ID\" column"
      ],
      "metadata": {
        "id": "2VvcJ69fraB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids"
      ],
      "metadata": {
        "id": "0gZ9zER9wRvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Drop the \"ID\" column from the \"test\" dataset to prepare the features\n",
        "test = test.drop(columns=['ID'])  # Remove \"ID\" from the dataset"
      ],
      "metadata": {
        "id": "o6BVRF5-r63q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    PorterStemmer()\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.word_tokenize(\"test sentence\")  # Check if 'punkt' is downloaded\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.pos_tag(nltk.word_tokenize(\"test sentence\"))  # Check if 'averaged_perceptron_tagger' is downloaded\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    nltk.help.upenn_tagset()  # Check if 'tagsets' is downloaded\n",
        "except LookupError:\n",
        "    nltk.download('tagsets')\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    text = re.sub(r'\\[|\\]', '', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    text = \" \".join(words)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def stem_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def add_tokens(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return \" \".join([f\"{word}/{tag}\" for word, tag in pos_tags])\n",
        "\n",
        "# Assuming 'train' and 'test' are your DataFrames\n",
        "stemmer = PorterStemmer()\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train[col] = train[col].apply(clean_text)\n",
        "    train[col] = train[col].apply(stem_text)\n",
        "    train[col] = train[col].apply(add_tokens)\n",
        "\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    test[col] = test[col].apply(clean_text)\n",
        "    test[col] = test[col].apply(stem_text)\n",
        "    test[col] = test[col].apply(add_tokens)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Label Encoding with Null Value Preservation\n",
        "def label_encode_with_nulls(train_df, test_df, column):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the encoder on the training data, handling nulls\n",
        "    # Combine unique values from both train and test to ensure all labels are seen\n",
        "    all_values = pd.concat([train_df[column], test_df[column]]).dropna().unique()\n",
        "    le.fit(all_values) # Fit on combined unique values\n",
        "\n",
        "    # Transform both train and test data, preserving nulls\n",
        "    train_df[column] = train_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "    test_df[column] = test_df[column].map(lambda x: le.transform([x])[0] if pd.notna(x) else x)\n",
        "    return train_df, test_df\n",
        "\n",
        "# Apply label encoding to object columns, preserving nulls\n",
        "for col in train.select_dtypes(include=['object']).columns:\n",
        "    train, test = label_encode_with_nulls(train, test, col)\n",
        "\n",
        "# Continue with your model training and prediction"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1Fb04h7xwjJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary library\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "YJ0flEuTxd2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ... (your existing code for preprocessing) ...\n",
        "\n",
        "# Label encode object columns in 'test' DataFrame\n",
        "for col in test.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    test[col] = le.fit_transform(test[col].astype(str))  # Convert to string before encoding\n",
        "\n",
        "# Convert 'test' DataFrame to numeric\n",
        "test = test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# ... (rest of your code for model prediction) ...\n",
        "\n",
        "# Predict on the test data using the trained LSTM model\n",
        "test_rnn = np.expand_dims(test, axis=1)\n",
        "\n",
        "# Convert to float32\n",
        "test_rnn = test_rnn.astype(np.float32) # convert the data to float32\n",
        "predictions = lstm_model.predict(test_rnn)\n",
        "\n",
        "# ... (rest of your code for creating submission file) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yoB4aZEQxkMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the submission file\n",
        "# submission_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "VHVVLJKXxK5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 4: Create a submission DataFrame using the stored \"ID\" and predictions\n",
        "submission_lstm = pd.DataFrame({\n",
        "    'ID': test_ids,      # Use the stored \"ID\" column\n",
        "    'matched_score': predictions[:, 1]  # Use the probabilities of the target class (class 1)\n",
        "})\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uqCJiuc-ydI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub"
      ],
      "metadata": {
        "id": "gE5pzYLFyzJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_lstm"
      ],
      "metadata": {
        "id": "FBgS0sDoymrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save the submission file to a CSV\n",
        "submission_lstm.to_csv('submission_lstm.csv', index=False)"
      ],
      "metadata": {
        "id": "aC3GFfBWykgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: same way predict for ANN\n",
        "\n",
        "# predicted_classes = ann_model.predict(test)\n",
        "\n",
        "# # Create a submission DataFrame\n",
        "# submission_ANN = pd.DataFrame({'ID': test_ids, 'matched_score': predicted_classes})\n",
        "\n",
        "# submission_ANN"
      ],
      "metadata": {
        "id": "ArPk5l1FykdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# prompt: same way predict for ANN\n",
        "\n",
        "predicted_probs = ann_model.predict(test)  # Get predicted probabilities\n",
        "\n",
        "# Assuming your predictions are probabilities, get the class with the highest probability\n",
        "# or select the probability for the relevant class (e.g., class 1)\n",
        "predicted_classes = predicted_probs[:, 1] # Change this index to 0 if it's binary classification\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission_ANN = pd.DataFrame({'ID': test_ids, 'matched_score': predicted_classes})\n",
        "\n",
        "submission_ANN"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "e8u4mrpz0h-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes"
      ],
      "metadata": {
        "id": "9SzrOoUzzYdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Now for RNN\n",
        "\n",
        "# Predict on the test data using the trained LSTM model\n",
        "test_rnn = np.expand_dims(test, axis=1)\n",
        "\n",
        "# Convert to float32\n",
        "test_rnn = test_rnn.astype(np.float32)  # convert the data to float32\n",
        "predictions = ann_model.predict(test)\n",
        "\n",
        "# Assuming your predictions are probabilities, get the class with the highest probability\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission_ANN = pd.DataFrame({'ID': test_ids, 'matched_score': y_test_pred_ANN[:1]})\n",
        "\n",
        "submission_ANN\n",
        "predicted_classes"
      ],
      "metadata": {
        "id": "guBCSKtfzawZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save the submission file to a CSV\n",
        "submission.to_csv('submission_lgbm_2.csv', index=False)"
      ],
      "metadata": {
        "id": "NUgxdU11rL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "z4kKRdFGrLzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "Xm6uNpbd1E9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann = ann_model.predict(test)"
      ],
      "metadata": {
        "id": "DsGpNZp31Tbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann"
      ],
      "metadata": {
        "id": "K68LlSf92AAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create a submission DataFrame\n",
        "submission_ANN = pd.DataFrame({'ID': test_ids, 'matched_score': ann[:, 1]}) # Select the second column (index 1) of y_test_pred_ANN\n",
        "\n",
        "submission_ANN"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2VuFMEy41xr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}